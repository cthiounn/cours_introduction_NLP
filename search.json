[
  {
    "objectID": "notebooks/2_Word_embeddings.html",
    "href": "notebooks/2_Word_embeddings.html",
    "title": "Transformer les mots en vecteurs numériques (Word embeddings)",
    "section": "",
    "text": "!pip install nltk gensim fasttext transformers torch sentence-transformers\nimport nltk\nnltk.download(\"punkt_tab\")\nTEXT=\"\"\"\nPréambule\n\nLes parties conviennent de rappeler que cet avenant temporaire n’a pas vocation à perdurer dans le temps, mais uniquement à répondre à une situation d’urgence dans nos établissements, principalement ceux ouverts en continu. Il s’applique pour une durée de 3 mois, à l’issue de laquelle une évaluation de ses effets sera réalisée.\n\nEn effet, depuis le début de la crise sanitaire en mars 2020, des mesures dérogatoires ont été appliquées dans nos établissements, dans le cadre de l’état d’urgence qui a pris fin au 31 juillet 2022.\n\nCes mesures ont permis de maintenir l’ouverture de nos établissements H24 et ainsi garantir le service que nous devons aux personnes que nous accompagnons au quotidien.\n\nCet avenant temporaire constitue un palliatif provisoire à la situation actuelle de pénurie de professionnels à l’échelle nationale et tous secteurs confondus qui représente la menace la plus importante au maintien de notre activité.\nEn parallèle des éléments définis dans le présent accord, l’ADAPEI s’engage, à l’instar des dispositions prises sur la MAS de Biganos il y a plusieurs mois, à porter une attention particulière au FO/FH de Martignas et d’apporter les solutions organisationnelles nécessaires à l’amélioration de la situation existante.\n\"\"\"\nTOKENS=nltk.tokenize.sent_tokenize(TEXT)"
  },
  {
    "objectID": "notebooks/2_Word_embeddings.html#one-hot-encoder",
    "href": "notebooks/2_Word_embeddings.html#one-hot-encoder",
    "title": "Transformer les mots en vecteurs numériques (Word embeddings)",
    "section": "1 - One-Hot Encoder",
    "text": "1 - One-Hot Encoder\n\n1 token = 1 vecteur canonique\n\n\nfrom sklearn.preprocessing import OneHotEncoder\nimport numpy as np\n\nwords=np.array([[\"rouge\"],[\"bleu\"],[\"vert\"]])\n\n# Initialize OneHotEncoder\nencoder = OneHotEncoder(sparse_output=False)\n\n# Fit and transform the data\nonehot_encoded = encoder.fit_transform(words)\n\nprint(onehot_encoded)\n\n\nDéfaut : tout est orthogonal/indépendant (alors que les synonymes sont proches)"
  },
  {
    "objectID": "notebooks/2_Word_embeddings.html#word2vec",
    "href": "notebooks/2_Word_embeddings.html#word2vec",
    "title": "Transformer les mots en vecteurs numériques (Word embeddings)",
    "section": "2 - Word2vec",
    "text": "2 - Word2vec\n\nUtilisation d’un réseau de neurones à trois couches pour modéliser les proba de prédire un mot en fonction de son contexte (cbow), ou un contexte en fonction d’un mot (skipgram)\n\n\nContinuous bag of words\ne.g :\n\nLe petit garçon ___ jouer à la balle avec le chien\n\naime / adore / préfère / déteste / haït\nfrom transformers import pipeline\nnlp = pipeline(\"fill-mask\", model=\"camembert-base\")\nnlp(f\"Le petit garçon {nlp.tokenizer.mask_token} jouer à la balle avec le chien.\")\n\n\n\n[{'score': 0.4084974527359009,\n  'token': 5817,\n  'token_str': 'adore',\n  'sequence': 'Le petit garçon adore jouer à la balle avec le chien.'},\n {'score': 0.1012398898601532,\n  'token': 1473,\n  'token_str': 'aime',\n  'sequence': 'Le petit garçon aime jouer à la balle avec le chien.'},\n {'score': 0.06638739258050919,\n  'token': 3051,\n  'token_str': 'préfère',\n  'sequence': 'Le petit garçon préfère jouer à la balle avec le chien.'},\n {'score': 0.061637673527002335,\n  'token': 198,\n  'token_str': 'va',\n  'sequence': 'Le petit garçon va jouer à la balle avec le chien.'},\n {'score': 0.02988985925912857,\n  'token': 604,\n  'token_str': 'veut',\n  'sequence': 'Le petit garçon veut jouer à la balle avec le chien.'}]\n\n\nLe petit garçon adore jouer à la _____ avec le chien\n\nballe / marelle / plage / campagne\nfrom transformers import pipeline\nnlp = pipeline(\"fill-mask\", model=\"camembert-base\")\nnlp(f\"Le petit garçon adore jouer à la {nlp.tokenizer.mask_token} avec le chien.\")\n\n\n[{'score': 0.1584215611219406,\n  'token': 5402,\n  'token_str': 'souris',\n  'sequence': 'Le petit garçon adore jouer à la souris avec le chien.'},\n {'score': 0.1299029439687729,\n  'token': 269,\n  'token_str': 'maison',\n  'sequence': 'Le petit garçon adore jouer à la maison avec le chien.'},\n {'score': 0.10889659821987152,\n  'token': 5417,\n  'token_str': 'balle',\n  'sequence': 'Le petit garçon adore jouer à la balle avec le chien.'},\n {'score': 0.058433305472135544,\n  'token': 3094,\n  'token_str': 'chasse',\n  'sequence': 'Le petit garçon adore jouer à la chasse avec le chien.'},\n {'score': 0.04963257908821106,\n  'token': 9862,\n  'token_str': 'poupée',\n  'sequence': 'Le petit garçon adore jouer à la poupée avec le chien.'}]\n Source : https://lhncbc.nlm.nih.gov/LSG/Projects/CSpell/docs/designDoc/UDF/Ranker/cbow.html\nA l’initialisation :\n\npoids aléatoire (de préférence non nuls)\nfonction d’activation de la couche cachée = Identité (une projection simple)\n\nA l’entraînement :\n\nForward : on prend chacun des tokens du contexte (c1, c2, c3, c4, etc.), on projette avec la couche cachée et on moyennise\nOn fait passer les résultats moyens de la couche cachée vers la couche de sortie\nOn passe des réels aux proba avec une softmax, on calcule la perte entre prédit et réel (une fonction de dirac)\nOn calcule les gradients et on fait la backpropagation\nOn itère un certain nombre d’étape (epochs)\n\nA “l’inférence” : * On ne fait pas de l’inférence pure, l’idée n’est plus de classifier * De ce fait, on enlève la couche de sortie * On s’intéresse à la couche cachée, qui nous fournit une réprésentation plus dense de notre vecteur canonique * Il suffit de faire le calcul matricielle, puisque la fonction est l’identité pour la couche cachée \\(Rep_w = I_m*C_w +b_m\\)\n\n\nSkipgram\n\nOn entraîne toujours un réseau de neurones à trois couches, même architecture, mais la méthode est différente, car on souhaite prédire à partir d’un mot son contexte probable\nPour cela, avec les données d’entraînement, on découpe en paires (mot, mot1_contexte); (mot, mot2_contexte) et essaie à partir du mot d’apprendre en moyenne les mots du contexte de manière indépendante\n“Le petit garçon aime jouer à la balle avec le chien” et mot pivot “aime”, à un contexte de deux mots = [(aime,garçon),(aime,petit),(aime,jouer),(aime,à)], cela donne quatre exemples indépendants\nMême chose, après entraînement, on enlève la couche de sortie pour obtenir les vecteurs projetés à partir de la couche cachée\n\n\n\nFrom scratch avec gensim\n\nTOKENS\n\n\ntokenized_sentences = [nltk.tokenize.word_tokenize(sentence.lower()) for sentence in TOKENS]\n\n# Print tokenized sentences\nprint(tokenized_sentences)\n\n\n#CBOW\nfrom gensim.models import Word2Vec\nmodel = Word2Vec(sentences=tokenized_sentences, vector_size=100, window=5, min_count=1, workers=4, sg=0)\nmodel.save(\"word2vec.model\")\n\n\n#Skipgram\nfrom gensim.models import Word2Vec\nmodel2 = Word2Vec(sentences=tokenized_sentences, vector_size=100, window=5, min_count=1, workers=4, sg=1)\nmodel2.save(\"word2vec2.model\")\n\n\nmodel.wv['palliatif']\n\n\nmodel2.wv['palliatif']\n\n\nsimilar_words = model.wv.most_similar('palliatif', topn=5)\nsimilar_words\n\n\nsimilar_words = model2.wv.most_similar('palliatif', topn=5)\nsimilar_words"
  },
  {
    "objectID": "notebooks/2_Word_embeddings.html#glove-un-concurrent",
    "href": "notebooks/2_Word_embeddings.html#glove-un-concurrent",
    "title": "Transformer les mots en vecteurs numériques (Word embeddings)",
    "section": "3 - Glove : un concurrent",
    "text": "3 - Glove : un concurrent\n\nWord2vec utilise la concordance, i.e. les mots directement proches aux alentours avec une taille de contexte\nL’idée de Glove est plutôt d’utiliser la cooccurrence, les mots qui occurent ensemble indépendemment de la distance dans la phrase\n\n\nfrom gensim.models import KeyedVectors\nimport gensim.downloader as api\n\nmodel = api.load(\"glove-wiki-gigaword-100\")\n\n# Load pre-trained Word2Vec model\n#path_to_model = 'GoogleNews-vectors-negative300.bin.gz'  # Path to the downloaded model\n#model = KeyedVectors.load_word2vec_format(path_to_model, binary=True)\n\n# Example: Get vector for 'king'\nprint(model['king'])\n\n\nprint(model['queen'])"
  },
  {
    "objectID": "notebooks/2_Word_embeddings.html#fasttext-word2vec-amélioré",
    "href": "notebooks/2_Word_embeddings.html#fasttext-word2vec-amélioré",
    "title": "Transformer les mots en vecteurs numériques (Word embeddings)",
    "section": "4 - Fasttext : Word2vec amélioré",
    "text": "4 - Fasttext : Word2vec amélioré\n\nMême chose que Word2vec, mais plutôt que les tokens soient des mots, les tokens sont des sous-mots/syllabes\nCela permet d’avoir une représentation vectorielle de mot qui n’était pas dans le vocabulaire initial\n\n\n!curl \"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.fr.300.bin.gz\" -o \"cc.fr.300.bin.gz\"\n\n\n!gunzip  \"cc.fr.300.bin.gz\" \n\n\nimport fasttext.util\n#fasttext.util.download_model('fr', if_exists='ignore')\nft = fasttext.load_model('cc.fr.300.bin')\n\n\nft.get_nearest_neighbors('Bonjour')\n\n\nft.get_word_vector('Bonjour')"
  },
  {
    "objectID": "notebooks/2_Word_embeddings.html#embedding-contextuel-bert-gpt-et-transformers",
    "href": "notebooks/2_Word_embeddings.html#embedding-contextuel-bert-gpt-et-transformers",
    "title": "Transformer les mots en vecteurs numériques (Word embeddings)",
    "section": "5 - Embedding contextuel : BERT / GPT et transformers",
    "text": "5 - Embedding contextuel : BERT / GPT et transformers\n\nUn mot peut signifier des choses différentes selon les contextes, selon la place ou rôle dans la phrase, selon l’article (in)défini précédent\nLe page != La page ; J’allume la cuisinière pour préparer un plat != Je discute avec la cuisinière ; L’opéra est à deux mètres != le chirurgien opéra le petit garçon\n(https://www.francaisfacile.com/exercices/exercice-francais-2/exercice-francais-3329.php)\nDe ce fait, avoir une réprésentation vectorielle pour un mot n’est pas suffisant, il faut en avoir plusieurs, en fonction des autres.\nJusqu’à présent, les algorithmes ci-dessus ne prennaient pas en compte l’ordre des mots (sac de mots)\n\n\n\n\nimage.png\n\n\n\nDans le cas de l’embedding de BERT, on prend un vocabulaire de taille n, on crée une projection vers une couche cachée de taille m &lt; n\nOn encode la position de 0 à L, et on crée une projection vers une couche cachée de taille m également\nOn encode également le type de token, s’il y a deux typologies dans une paire par exemple (“model”,“humain”) et on crée une projection vers une couche cachée de taille m\nLa représentation vectorielle d’un token de la phrase est alors la somme des trois !\nAinsi, la représentation vectorielle du mot “cuisinière” est différente pour “J’allume la cuisinière pour préparer un plat” et pour “Je discute avec la cuisinière”\nOn a alors une représentation vectorielle dynamique et contextuelle\nLe petit chien joue avec le chat ; Le chien joue avec le petit chat\n\n\nimport torch\nfrom transformers import BertTokenizer, BertModel\n\n# Load pre-trained model and tokenizer\nmodel_name = 'bert-base-uncased'\ntokenizer = BertTokenizer.from_pretrained(model_name)\nmodel = BertModel.from_pretrained(model_name)\n\n# Input text\ntext = \"Le petit chien joue avec le chat\"\n\n# Tokenize the input text\ninputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n\ninputs\n\n\ntokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0].tolist())\ntokens\n\n\nwith torch.no_grad():\n    outputs = model(**inputs)\n\nlast_hidden_states = outputs.last_hidden_state\n\npetit_embedding = last_hidden_states[:, 2, :]\n\n\n# Input text\ntext = \"Le chien joue avec le petit chat\"\n\n# Tokenize the input text\ninputs2 = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n\ninputs2\n\n\ntokens = tokenizer.convert_ids_to_tokens(inputs2[\"input_ids\"][0].tolist())\ntokens\n\n\nwith torch.no_grad():\n    outputs2 = model(**inputs2)\n\nlast_hidden_states2 = outputs2.last_hidden_state\n\npetit_embedding2 = last_hidden_states2[:, 9, :]\n\n\npetit_embedding[0,:10]\n\n\npetit_embedding2[0,:10]\n\n\npositional_embeddings = model.embeddings.position_embeddings\npositional_embeddings.weight.data[0,:10]\n\n\ntoken_type_embeddings = model.embeddings.token_type_embeddings\ntoken_type_embedding_values = token_type_embeddings.weight.data\ntoken_type_embedding_values"
  },
  {
    "objectID": "notebooks/2_Word_embeddings.html#passer-du-mot-à-la-phrase",
    "href": "notebooks/2_Word_embeddings.html#passer-du-mot-à-la-phrase",
    "title": "Transformer les mots en vecteurs numériques (Word embeddings)",
    "section": "6 - Passer du mot à la phrase",
    "text": "6 - Passer du mot à la phrase\n\nmoyenne (fasttext), max, somme des vecteurs\nSentenceBert (SBERT), entraîné à la tâche de ‘Semantic Textual Similarity’\n\n\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Load the model\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\n\n# List of sentences\nsentences = [\n    \"Le chien ne joue pas avec le petit chat\",\n    \"Le petit chien joue avec le chat\"\n]\n\n# Generate embeddings\nembeddings = model.encode(sentences)\n\n# Calculate similarity\nsimilarity = cosine_similarity([embeddings[0]], [embeddings[1]])\nprint(\"Similarity between sentence 1 and 2:\", similarity[0][0])"
  },
  {
    "objectID": "ressources.html",
    "href": "ressources.html",
    "title": "Cours introduction au NLP",
    "section": "",
    "text": "https://www.cri.ensmp.fr/~silber/nlp_archives/202406_intro_nlp.pdf\nhttps://www-inf.telecom-sudparis.eu/COURS/CSC4538/Supports/cours/intro_nlp.pdf\nhttps://cours-machine-learning.blogspot.com/p/nlp-web-mining.html\nhttps://course.spacy.io/fr/\nhttps://www.stat4decision.com/fr/traitement-langage-naturel-francais-tal-nlp/\nhttps://oku.ozturkibrahim.com/docs_python/Deep_Learning_for_Natural_Language_Processing.pdf\nhttps://karczmarczuk.users.greyc.fr/TEACH/TAL/Doc/Handbook%20Of%20Natural%20Language%20Processing,%20Second%20Edition%20Chapman%20&%20Hall%20Crc%20Machine%20Learning%20&%20Pattern%20Recognition%202010.pdf\nhttps://github.com/msd495/machine-learning-pdf-books/blob/master/Deep%20Learning%20for%20Natural%20Language%20Processing.pdf"
  },
  {
    "objectID": "notebooks/NLTK.html",
    "href": "notebooks/NLTK.html",
    "title": "Séance 1 - Caractérisation du texte : approche fréquentiste - Préparer le texte et compter les mots",
    "section": "",
    "text": "!pip install -q nltk seaborn pandas WordCloud\nTEXT=\"\"\"   \nACCORD D’ENTREPRISE \nPrime Jeux Olympiques et Paralympiques\nAIRLINES GROUND SERVICES\nDu 11 juillet 2024 \n\n\nEntre les soussignés :\n\nLa société AIRLINES GROUND SERVICES au capital de 38 112 €, inscrite au registre du commerce et des sociétés de Bobigny sous le numéro 411 545 080, dont le siège social est situé à Tremblay en France (Seine Saint Denis) au 3 rue du Remblai, représentée par Xxx, agissant en qualité de Président, \n\nCi-après dénommée la « Société »\n\nD’UNE PART \n\nET \n\nLes organisations syndicales représentées par : \n\nXxx pour la CGT, délégué syndical,\nXxx pour le SMA, délégué syndical,\nXxx pour la CFTC, délégué syndical,\nXxx pour la CFE-CGC, délégué syndical\nXxx pour FO, délégué syndical\n\nCi-après dénommées les « Organisations Syndicales » \n\nD’AUTRE PART\n\nCi-après dénommées ensemble « les Parties »\n\nAfin de répondre aux revendications salariales du personnel de la société AIRLINES GROUND SERVICES et dans le cadre de l’évènement des Jeux Olympiques et Paralympiques organisés en France et le travail des salariés pendant cette période, il a été convenu et arrêté les points suivants : \n\n\nArticle 1 – Champ d’application\n\nLe présent accord s’applique à l’ensemble du personnel salarié de la société AIRLINES GROUND SERVICES.\n\nArticle 2 – Portée de l’accord\n\nLe présent accord est conclu dans le cadre des textes en vigueur du Code du travail. \nCet accord mettant en place des dispositions plus favorables que celles prévues actuellement par la Convention Collective Transport Aérien Personnel au Sol n° 3177, il s’y substitue en ce qui concerne la grille des minimas hiérarchiques.\n\n\n\"\"\"\nimport nltk\n# Module pour les ponctuations, pour savoir comment découper \"M. XXX a trois chats. Il a également deux chiens.\" =&gt; [\"M.\", \"XXX\", ...]\nnltk.download('punkt_tab')\n# Module pour les stopwords\nnltk.download('stopwords')\n# Module pour lemmatiser\nnltk.download('wordnet')"
  },
  {
    "objectID": "notebooks/NLTK.html#séance-2---caractérisation-du-texte---compter-les-mots-par-ensemble---ngrams",
    "href": "notebooks/NLTK.html#séance-2---caractérisation-du-texte---compter-les-mots-par-ensemble---ngrams",
    "title": "Séance 1 - Caractérisation du texte : approche fréquentiste - Préparer le texte et compter les mots",
    "section": "Séance 2 - Caractérisation du texte - Compter les mots par ensemble - Ngrams",
    "text": "Séance 2 - Caractérisation du texte - Compter les mots par ensemble - Ngrams\n\nfrom nltk import ngrams\n\ntokens = words_without_stop_words_and_punctuation\n# Generate bigrams (2-grams)\nbigrams = list(ngrams(tokens, 2))\n\n# Print the bigrams\nprint(tokens)\nprint(bigrams)\n\n# You can also generate trigrams or higher n-grams\ntrigrams = list(ngrams(tokens, 3))\nprint(trigrams)\n\n# For any n-gram, simply change the second parameter of the ngrams function\nn = 4  # Example for 4-grams\nfourgrams = list(ngrams(tokens, n))\nprint(fourgrams)\n\n\nfrom nltk.probability import FreqDist\n\n# Create bigrams\nbigrams = list(ngrams(tokens, 2))\n\n# Count the frequency of bigrams\nfdist = FreqDist(bigrams)\n\n# Print the most common bigrams\nprint(fdist.most_common(5))\n\n\nimport pandas as pd\nimport seaborn as sns\n\n# Convert the frequency distribution to a pandas DataFrame\nbigram_freq_df = pd.DataFrame(fdist.items(), columns=['Bigram', 'Frequency'])\n\n# Split the bigram tuples into separate columns for better handling\nbigram_freq_df[['Word1', 'Word2']] = pd.DataFrame(bigram_freq_df['Bigram'].tolist(), index=bigram_freq_df.index)\n\n# Remove the original 'Bigram' column as we no longer need it\nbigram_freq_df.drop(columns=['Bigram'], inplace=True)\n\n# Sort by frequency to get the most common bigrams\nbigram_freq_df = bigram_freq_df.sort_values(by='Frequency', ascending=False)\n\n# Create a new column for better labeling in the plot (e.g., 'Word1 Word2')\nbigram_freq_df['Bigram'] = bigram_freq_df['Word1'] + ' ' + bigram_freq_df['Word2']\n\n# Plot the top 10 bigrams using Seaborn\nplt.figure(figsize=(10, 6))\nsns.barplot(x='Frequency', y='Bigram', data=bigram_freq_df.head(10), hue='Bigram', palette='viridis', dodge=False, legend=False)\nplt.title('Top 10 Bigrams by Frequency')\nplt.xlabel('Frequency')\nplt.ylabel('Bigrams')\nplt.show()\n\n\nfrom nltk.probability import FreqDist\n\n# Create bigrams\nbigrams = list(ngrams(tokens, 3))\n\n# Count the frequency of bigrams\nfdist = FreqDist(bigrams)\n\n# Print the most common bigrams\nprint(fdist.most_common(5))\n\n\nfdist.plot(10)"
  },
  {
    "objectID": "notebooks/NLTK.html#séance-3---caractérisation-du-texte---analyser-les-cooccurrences-et-les-concordances",
    "href": "notebooks/NLTK.html#séance-3---caractérisation-du-texte---analyser-les-cooccurrences-et-les-concordances",
    "title": "Séance 1 - Caractérisation du texte : approche fréquentiste - Préparer le texte et compter les mots",
    "section": "Séance 3 - Caractérisation du texte - Analyser les cooccurrences et les concordances",
    "text": "Séance 3 - Caractérisation du texte - Analyser les cooccurrences et les concordances\n\nConcordance : mots avant et après (contexte)\n\ntext_nltk = nltk.Text(words_without_stop_words_and_punctuation)\n\ntext_nltk.concordance('paralympiques')\n\n\ntext_nltk.concordance('jeux')\n\n\ntext_nltk.concordance('france')\n\n\ntext_nltk.concordance('salarié')\n\n\n\nCo-occurences : Mots qui apparaissent ensemble dans les phrases\n\npas de fonction dans NTLK pour faire cela, hormis les n-grams"
  }
]